{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d6e9f4",
   "metadata": {},
   "source": [
    "# Processing CSV data using LLMs on Amazon Bedrock\n",
    "\n",
    "[Semi-structured data](https://en.wikipedia.org/wiki/Semi-structured_data) is a form of structured data that does not obey the tabular structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data.\n",
    "\n",
    "In complex [Generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence) use cases that involve [Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model), we often come across the need to process semi-structured data through LLMs.\n",
    "\n",
    "This notebook will walk you through examples of processing [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) data through natural language queries by using the LLMs hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/). These would be,\n",
    "* Data extraction with conditions\n",
    "* Filtering\n",
    "* Aggregation\n",
    "* Sorting\n",
    "* Transformations\n",
    "\n",
    "We will use [LangChain](https://www.langchain.com/) to simplify the process of constructing the prompts and interacting with the LLMs. In the process of working through this notebook, you will learn how to setup the Amazon Bedrock client environment, configure security permissions and use prompt templates in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515125d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> LLMs are not a good fit for some of these operations as you will notice in the prompt responses further down in the notebook. For those, you will be better off performing those operations outside the LLMs and passing the results to the LLMs for further processing. One way to achieve this is through <a href=\"https://python.langchain.com/docs/modules/model_io/chat/function_calling\">function calling</a> but that is out of scope for this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111293",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>This notebook should only be run from within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">Amazon SageMaker Notebook instance</a> or within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated.html\">Amazon SageMaker Studio Notebook</a>.</li>\n",
    "        <li>This notebook uses text based models along with their versions that were available at the time of writing. Update these as required.</li>\n",
    "        <li>At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. Follow the guidance in the <i>Organize imports</i> section of this notebook.</li>\n",
    "        <li>This notebook is recommended to be run with a minimum instance size of <i>ml.m5.xlarge</i> and\n",
    "            <ul>\n",
    "                <li>With <i>Amazon Linux 2, Jupyter Lab 3</i> as the platform identifier on an Amazon SageMaker Notebook instance.</li>\n",
    "                <li> (or)\n",
    "                <li>With <i>Data Science 3.0</i> as the image on an Amazon SageMaker Studio Notebook.</li>\n",
    "            <ul>\n",
    "        </li>\n",
    "        <li>At the time of this writing, the most relevant latest version of the Kernel for running this notebook,\n",
    "            <ul>\n",
    "                <li>On an Amazon SageMaker Notebook instance was <i>conda_python3</i></li>\n",
    "                <li>On an Amazon SageMaker Studio Notebook was <i>Python 3</i></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e88d20",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Install required software libraries](#Install%20required%20software%20libraries)\n",
    "    \n",
    "    3. [Configure logging](#Configure%20logging)\n",
    "        \n",
    "        1. [System logs](#Configure%20system%20logs)\n",
    "        \n",
    "        2. [Application logs](#Configure%20application%20logs)\n",
    "    \n",
    "    4. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    5. [Create common objects](#Create%20common%20objects)\n",
    "    \n",
    "    6. [Enable model access in Amazon Bedrock](#Enable%20model%20access%20in%20Amazon%20Bedrock)\n",
    "    \n",
    "    7. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "    \n",
    "    8. [List the available models](#List%20the%20available%20models)\n",
    "\n",
    " 2. [Prompt examples](#Prompt%20examples)\n",
    " \n",
    "    1. [Prompt 1](#Prompt%201)\n",
    "     \n",
    "        1. [AI21 Labs Jurassic](#AI21%20Labs%20Jurassic%20prompt%201)\n",
    "        \n",
    "        2. [Anthropic Claude](#Anthropic%20Claude%20prompt%201)\n",
    "        \n",
    "        3. [Cohere Command](#Cohere%20Command%20prompt%201)\n",
    "        \n",
    "        4. [Mistral](#Mistral%20prompt%201)\n",
    "        \n",
    "        5. [Meta LLAMA 2](#Meta%20LLAMA%202%20prompt%201)\n",
    "        \n",
    "        6. [Amazon Titan](#Amazon%20Titan%20prompt%201)\n",
    " \n",
    " 3. [Cleanup](#Cleanup)\n",
    " \n",
    " 4. [Conclusion](#Conclusion)\n",
    " \n",
    " 5. [Frequently Asked Questions (FAQs)](#FAQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb9d3",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id ='Complete%20prerequisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85c39b",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id ='Check%20and%20configure%20access%20to%20the%20Internet'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through an [Amazon VPC](https://aws.amazon.com/vpc/).  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efd56",
   "metadata": {},
   "source": [
    "### B. Install required software libraries <a id ='Install%20required%20software%20libraries'> </a>\n",
    "This notebook requires the following libraries:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "* [LangChain](https://www.langchain.com/)\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb373af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the end of the installation, the Kernel will be forcefully restarted immediately. Please wait 10 seconds for the kernel to come back before running the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d4fc-0361-4cee-a548-d9b7e355824f",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.34.62\n",
    "!pip install langchain==0.1.12\n",
    "!pip install sagemaker==2.212.0\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3c44f",
   "metadata": {},
   "source": [
    "### C. Configure logging <a id ='Configure%20logging'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ee37",
   "metadata": {},
   "source": [
    "####  a. System logs <a id='Configure%20system%20logs'></a>\n",
    "\n",
    "System logs refers to the logs generated by the notebook's interactions with the underlying notebook instance. Some examples of these are the logs generated when loading or saving the notebook.\n",
    "\n",
    "These logs are automatically setup when the notebook instance is launched.\n",
    "\n",
    "These logs can be accessed through the [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html) console in the same AWS Region where this notebook is running.\n",
    "* When running this notebook in an Amazon SageMaker Notebook instance, navigate to the following location,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/NotebookInstances > {notebook-instance-name}/jupyter.log</i>\n",
    "* When running this notebook in an Amazon SageMaker Studio Notebook, navigate to the following locations,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/KernelGateway/{notebook-instance-name}</i>\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/JupyterServer/default</i>\n",
    "\n",
    "Run the following cell to print the name of the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "notebook_name = ''\n",
    "resource_metadata_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "with open(resource_metadata_path, 'r') as metadata:\n",
    "    notebook_name = (json.load(metadata))['ResourceName']\n",
    "print(\"Notebook instance name: '{}'\".format(notebook_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc0025",
   "metadata": {},
   "source": [
    "####  b. Application logs <a id='Configure%20application%20logs'></a>\n",
    "\n",
    "Application logs refers to the logs generated by running the various code cells in this notebook. To set this up, instantiate the [Python logging service](https://docs.python.org/3/library/logging.html) by running the following cell. You can configure the default log level and format as required.\n",
    "\n",
    "By default, this notebook will only print the logs to the corresponding cell's output console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the logging level and format\n",
    "log_level = logging.INFO\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=log_level, format=log_format)\n",
    "\n",
    "# Save these in the environment variables for use in the helper scripts\n",
    "os.environ['LOG_LEVEL'] = str(log_level)\n",
    "os.environ['LOG_FORMAT'] = log_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb063",
   "metadata": {},
   "source": [
    "###  D. Organize imports <a id ='Organize%20imports'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a06b9-812c-4dad-a652-1cb34aa9d8b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import langchain\n",
    "import sagemaker\n",
    "import sys\n",
    "from botocore.config import Config\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#logging.info(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba26b",
   "metadata": {},
   "source": [
    "Print the installed versions of some of the important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"Python version : {}\".format(sys.version))\n",
    "logging.info(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "logging.info(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))\n",
    "logging.info(\"LangChain version : {}\".format(langchain.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f63309",
   "metadata": {},
   "source": [
    "###  E. Create common objects <a id='Create%20common%20objects'> </a>\n",
    "\n",
    "Get the current AWS Region (where this notebook is running) and the SageMaker Session. These will be used to initialize some of the clients to AWS services using the boto3 APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7be30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. In order to do this, this notebook will use the value specified in the environment variable named <mark>AMAZON_BEDROCK_REGION</mark>. If this is not specified, then the notebook will default to <mark>us-west-2 (Oregon)</mark> for Amazon Bedrock.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS Region, SageMaker Session and IAM Role references\n",
    "my_session = boto3.session.Session()\n",
    "logging.info(\"SageMaker Session: {}\".format(my_session))\n",
    "my_iam_role = sagemaker.get_execution_role()\n",
    "logging.info(\"Notebook IAM Role: {}\".format(my_iam_role))\n",
    "my_region = my_session.region_name\n",
    "logging.info(\"Current AWS Region: {}\".format(my_region))\n",
    "\n",
    "# Explicity set the AWS Region for Amazon Bedrock clients\n",
    "AMAZON_BEDROCK_DEFAULT_REGION = \"us-west-2\"\n",
    "br_region = os.environ.get('AMAZON_BEDROCK_REGION')\n",
    "if br_region is None:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "elif len(br_region) == 0:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "logging.info(\"AWS Region for Amazon Bedrock: {}\".format(br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8484fc",
   "metadata": {},
   "source": [
    "Set the timeout and retry configurations that will be applied to all the boto3 clients used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the standard time out limits in the boto3 client from 1 minute to 3 minutes\n",
    "# and set the retry limits\n",
    "my_boto3_config = Config(\n",
    "    connect_timeout = (60 * 3),\n",
    "    read_timeout = (60 * 3),\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1322f",
   "metadata": {},
   "source": [
    "Create the rest of the common objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693def8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Amazon Bedrock client\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name = br_region, endpoint_url = \"https://bedrock.{}.amazonaws.com\"\n",
    "                              .format(br_region), config = my_boto3_config)\n",
    "\n",
    "# Create the Amazon Bedrock runtime client\n",
    "bedrock_rt_client = boto3.client(\"bedrock-runtime\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "# Specify the path to the directories that will contain the prompt\n",
    "# templates and data for the prompts\n",
    "prompt_1_templates_dir = os.path.join(os.getcwd(), \"prompt_templates\", \"csv\", \"prompt1\")\n",
    "prompt_2_templates_dir = os.path.join(os.getcwd(), \"prompt_templates\", \"csv\", \"prompt2\")\n",
    "data_dir = os.path.join(os.getcwd(), \"data\", \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f0e97",
   "metadata": {},
   "source": [
    "###  F. Enable model access in Amazon Bedrock <a id ='Enable%20model%20access%20in%20Amazon%20Bedrock'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84ba9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> Before invoking any model in Amazon Bedrock, enable access to that model by following the instructions <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">here</a>. In addition, for Anthropic models, you need to submit the use case details. Otherwise, you will get an authorization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de160b",
   "metadata": {},
   "source": [
    "Run the following cell to print the Amazon Bedrock model access page URL for the AWS Region that was selected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78213222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Amazon Bedrock model access page URL\n",
    "logging.info(\"Amazon Bedrock model access page - https://{}.console.aws.amazon.com/bedrock/home?region={}#/modelaccess\"\n",
    "             .format(br_region, br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57899f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> You will have to do this manually after reading the End User License Agreement (EULA) for each of the models that you want to enable. Unless you explicitly disable it, this is a one-time setup for each model in an AWS account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee077",
   "metadata": {},
   "source": [
    "###  G. Check and configure security permissions <a id ='Check%20and%20configure%20security%20permissions'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell.\n",
    "\n",
    "This IAM role should have the following permissions,\n",
    "\n",
    "1. Full access to invoke Large Language Models (LLMs) on Amazon Bedrock.\n",
    "2. Access to write to Amazon CloudWatch Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16baab7",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the IAM role attached to the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the IAM role ARN and console URL\n",
    "logging.info(\"This notebook's IAM role is '{}'\".format(my_iam_role))\n",
    "arn_parts = my_iam_role.split('/')\n",
    "logging.info(\"Details of this IAM role are available at https://{}.console.aws.amazon.com/iamv2/home?region={}#/roles/details/{}?section=permissions\"\n",
    "             .format(my_region, my_region, arn_parts[len(arn_parts) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12579ad7",
   "metadata": {},
   "source": [
    "###  H. List the available models <a id='List%20the%20available%20models'> </a>\n",
    "\n",
    "Running the following cell will list all available LLMs on Amazon Bedrock that have 'TEXT' as at least one of the input and output modalities. The results be will filtered further to show only those LLMs that are offered through the On-Demand throughput pricing model. This will help you pick the model-ids that you will use further down in this notebook.\n",
    "\n",
    "For more information on this, refer [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e08e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all the available text based LLMs in Amazon Bedrock with On-Demand throughput pricing\n",
    "models_info = ''\n",
    "response = bedrock_client.list_foundation_models(byOutputModality = \"TEXT\", byInferenceType = \"ON_DEMAND\")\n",
    "model_summaries = response[\"modelSummaries\"]\n",
    "models_info = models_info + \"\\n\"\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(\"Provider Name\", \"Model Name\", \"Input Modalities\",\n",
    "                                                          \"Output Modalities\", \"Model Id\")\n",
    "models_info = models_info + \"-\".ljust(125, \"-\")\n",
    "for model_summary in model_summaries:\n",
    "    # Check for 'TEXT' modality in both input and output (./scripts/helper_functions.py) and process\n",
    "    if does_modality_exists(model_summary[\"inputModalities\"],\n",
    "                            model_summary[\"outputModalities\"], 'TEXT'):\n",
    "        models_info = models_info + \"\\n\"\n",
    "        models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(model_summary[\"providerName\"],\n",
    "                                                                                model_summary[\"modelName\"],\n",
    "                                                                                \"|\".join(model_summary[\"inputModalities\"]),\n",
    "                                                                                \"|\".join(model_summary[\"outputModalities\"]),\n",
    "                                                                                model_summary[\"modelId\"])\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "logging.info(\"Displaying available models in the '{}' Region:\".format(br_region) + models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5d414",
   "metadata": {},
   "source": [
    "## 2. Prompt examples <a id ='Prompt%20examples'> </a>\n",
    "\n",
    "In this section, we will take some sample CSV files and construct prompts that will ask questions or provide instructions on what we want to lookup from them. We will also show examples of how to get the LLMs to perform data extraction, sorting, aggregation and filtering operations on the CSV data.\n",
    "\n",
    "In our examples, we want the LLMs to generate the most probable response. So we will set the `temperature` to `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba29e2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Constructing the optimal prompt for your requirement is both a science and an art. It involes experimentation and iteration. To start with, always refer to the model provider's documentation to get an idea of the constructs and best practices for prompting their models. Here are the links:\n",
    "    <ul>\n",
    "        <li>AI21 Labs Jurassic models, refer <a href=\"https://docs.ai21.com/docs/prompt-engineering\">here</a>.</li>\n",
    "        <li>Anthropic Claude models, refer <a href=\"https://docs.anthropic.com/claude/docs/constructing-a-prompt\">here</a>.</li>\n",
    "        <li>Cohere Command models, refer <a href=\"https://txt.cohere.com/constructing-prompts/\">here</a>.</li>\n",
    "        <li>Mistral models, refer <a href=\"https://docs.mistral.ai/guides/prompting-capabilities/\">here</a>.</li>\n",
    "        <li>Meta LLAMA 2 models, refer <a href=\"https://llama.meta.com/get-started#prompting\">here</a>.</li>\n",
    "        <li>Amazon Titan models, refer <a href=\"https://d2eo22ngex1n9g.cloudfront.net/Documentation/User+Guides/Titan/Amazon+Titan+Text+Prompt+Engineering+Guidelines.pdf\">here</a>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d057297",
   "metadata": {},
   "source": [
    "### A. Prompt 1 <a id ='Prompt%201'> </a>\n",
    "\n",
    "In this prompt,\n",
    "\n",
    "* We will take a CSV file named `books.csv` that contains data about some books.\n",
    "* This will be a [zero-shot prompt](https://www.promptingguide.ai/techniques/zeroshot).\n",
    "* The call-to-action i.e. the ask from the LLM,\n",
    "    * Will be a single turn conversation and not a back-and-forth conversation.\n",
    "    * Will be questions from that CSV file. These questions will be in natural language and the responses from the LLM will either be natural language or a CSV based on the question. You will see that the LLM will need to parse the CSV, loop through items, perform some math calculations and read the descriptions in order to respond. \n",
    "    * Will be instructions for the LLM to convert that CSV to other data formats.  \n",
    "\n",
    "Note that some books are sequels to the other books listed in that CSV. Where applicable, the LLMs should pay attention to this for an effective response. For example, observe the response to the call-to-action `\"I am interested in reading about the aftermath of the fall of nanotechnology. What should I read?\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f27dd",
   "metadata": {},
   "source": [
    "To get started, view the `books.csv` file by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadff167",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat data/csv/books.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23fd0f",
   "metadata": {},
   "source": [
    "Now set the inference parameters and the call-to-action.\n",
    "\n",
    "Uncomment the various `call_to_action` lines in the below cell one at a time and try out each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdfc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the inference parameters\n",
    "temperature = 0\n",
    "max_response_token_length = -1\n",
    "\n",
    "# Read the data (./scripts/helper_functions.py) from the file\n",
    "# that contains the CSV data to be used in this prompt\n",
    "csv_data = read_file(data_dir, 'books.csv')\n",
    "\n",
    "#### Data extraction with conditions (NOTE: You will see accurate results almost all the time)\n",
    "#call_to_action = 'get me the book snippet where Cynthia Randall is the author.'\n",
    "call_to_action = 'what is the newest publication?'\n",
    "#call_to_action = 'what is the the cheapest book?'\n",
    "#call_to_action = 'what is the oldest title?'\n",
    "#call_to_action = 'what is the price of Jungle Book?'\n",
    "\n",
    "#### Filtering (NOTE: You will see accurate results almost all the time)\n",
    "#call_to_action = 'get me the snippets of all computer related books.'\n",
    "#call_to_action = 'I am interested in reading about the aftermath of the fall of nanotechnology. What should I read?'\n",
    "#call_to_action = 'what are the titles of the cheapest books?'\n",
    "\n",
    "#### Aggregation (NOTE: Except for the total cost call-to-action, you will see accurate results for others almost all the time)\n",
    "#call_to_action = 'how many books are there in the catalog?'\n",
    "#call_to_action = 'get all titles that belong to the fantasy genre.'\n",
    "#call_to_action = 'which author has the most titles?'\n",
    "#call_to_action = 'I want to buy all the books. How much will it cost?'\n",
    "\n",
    "#### Sorting (NOTE: You will NOT see accurate results most of the time)\n",
    "#call_to_action = 'consider every line as a record. Then, sort the records in ascending order of publication date.'\n",
    "#call_to_action = 'consider every line as a record. Then, sort the records in descending order of price.'\n",
    "\n",
    "#### Transformations (NOTE: You will see accurate results almost all the time)\n",
    "#call_to_action = 'convert this CSV to a HTML table.'\n",
    "#call_to_action = 'convert this CSV to XML.'\n",
    "#call_to_action = 'convert this CSV to JSON.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064bbbb",
   "metadata": {},
   "source": [
    "####  A. AI21 Labs Jurassic <a id ='AI21%20Labs%20Jurassic%20prompt%201'> </a>\n",
    "\n",
    "View the prompt template by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prompt_templates/csv/prompt1/AI21_Labs_Jurassic_prompt1_for_CSV.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701621a",
   "metadata": {},
   "source": [
    "Now select the model-id, run the following cell and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24474dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model-id\n",
    "#model_id = \"ai21.j2-mid-v1\"\n",
    "model_id = \"ai21.j2-ultra-v1\"\n",
    "\n",
    "# Prepare the prompt and invoke the LLM (./scripts/helper_functions.py)\n",
    "single_turn_conversation = process_prompt_1(model_id, bedrock_rt_client, temperature, max_response_token_length,\n",
    "                                            prompt_1_templates_dir, 'AI21_Labs_Jurassic_prompt1_for_CSV.txt',\n",
    "                                            csv_data, call_to_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169de5b6",
   "metadata": {},
   "source": [
    "####  B. Anthropic Claude <a id ='Anthropic%20Claude%20prompt%201'> </a>\n",
    "\n",
    "View the prompt template by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prompt_templates/csv/prompt1/Anthropic_Claude_prompt1_for_CSV.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3686708",
   "metadata": {},
   "source": [
    "Now select the model-id, run the following cell and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69579ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model-id\n",
    "#model_id = \"anthropic.claude-instant-v1\"\n",
    "#model_id = \"anthropic.claude-v2\"\n",
    "#model_id = \"anthropic.claude-v2:1\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Prepare the prompt and invoke the LLM (./scripts/helper_functions.py)\n",
    "single_turn_conversation = process_prompt_1(model_id, bedrock_rt_client, temperature, max_response_token_length,\n",
    "                                            prompt_1_templates_dir, 'Anthropic_Claude_prompt1_for_CSV.txt',\n",
    "                                            csv_data, call_to_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37756121",
   "metadata": {},
   "source": [
    "####  C. Cohere Command <a id ='Cohere%20Command%20prompt%201'> </a>\n",
    "\n",
    "View the prompt template by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b68372",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prompt_templates/csv/prompt1/Cohere_Command_prompt1_for_CSV.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998d0df",
   "metadata": {},
   "source": [
    "Now select the model-id, run the following cell and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model-ids along with their inference parameters\n",
    "#model_id = \"cohere.command-light-text-v14\"\n",
    "model_id = \"cohere.command-text-v14\"\n",
    "\n",
    "# Prepare the prompt and invoke the LLM (./scripts/helper_functions.py)\n",
    "single_turn_conversation = process_prompt_1(model_id, bedrock_rt_client, temperature, max_response_token_length,\n",
    "                                            prompt_1_templates_dir, 'Cohere_Command_prompt1_for_CSV.txt',\n",
    "                                            csv_data, call_to_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ea2bf",
   "metadata": {},
   "source": [
    "####  D. Mistral <a id ='Mistral%20prompt%201'> </a>\n",
    "\n",
    "View the prompt template by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7918bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prompt_templates/csv/prompt1/Mistral_prompt1_for_CSV.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da21b7",
   "metadata": {},
   "source": [
    "Now select the model-id, run the following cell and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a806be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model-ids along with their inference parameters\n",
    "#model_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "\n",
    "# Prepare the prompt and invoke the LLM (./scripts/helper_functions.py)\n",
    "single_turn_conversation = process_prompt_1(model_id, bedrock_rt_client, temperature, max_response_token_length,\n",
    "                                            prompt_1_templates_dir, 'Mistral_prompt1_for_CSV.txt',\n",
    "                                            csv_data, call_to_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814df402",
   "metadata": {},
   "source": [
    "####  E. Meta LLAMA 2 <a id ='Meta%20LLAMA%202%20prompt%201'> </a>\n",
    "\n",
    "View the prompt template by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prompt_templates/csv/prompt1/Meta_LLAMA_2_prompt1_for_CSV.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072ec2a",
   "metadata": {},
   "source": [
    "Now select the model-id, run the following cell and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model-ids along with their inference parameters\n",
    "#model_id = \"meta.llama2-13b-chat-v1\"\n",
    "model_id = \"meta.llama2-70b-chat-v1\"\n",
    "\n",
    "# Prepare the prompt and invoke the LLM (./scripts/helper_functions.py)\n",
    "single_turn_conversation = process_prompt_1(model_id, bedrock_rt_client, temperature, max_response_token_length,\n",
    "                                            prompt_1_templates_dir, 'Meta_LLAMA_2_prompt1_for_CSV.txt',\n",
    "                                            csv_data, call_to_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b55739e",
   "metadata": {},
   "source": [
    "####  F. Amazon Titan <a id ='Amazon%20Titan%20prompt%201'> </a>\n",
    "\n",
    "View the prompt template by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat prompt_templates/csv/prompt1/Amazon_Titan_prompt1_for_CSV.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa10484",
   "metadata": {},
   "source": [
    "Now select the model-id, run the following cell and observe the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ae1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model-ids along with their inference parameters\n",
    "#model_id = \"amazon.titan-text-lite-v1\"\n",
    "model_id = \"amazon.titan-text-express-v1\"\n",
    "\n",
    "# Prepare the prompt and invoke the LLM (./scripts/helper_functions.py)\n",
    "single_turn_conversation = process_prompt_1(model_id, bedrock_rt_client, temperature, max_response_token_length,\n",
    "                                            prompt_1_templates_dir, 'Amazon_Titan_prompt1_for_CSV.txt',\n",
    "                                            csv_data, call_to_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1216a",
   "metadata": {},
   "source": [
    "## 3. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "As a best practice, you should delete AWS resources that are no longer required.  This will help you avoid incurring unncessary costs.\n",
    "\n",
    "The minimum cleanup required for this notebook would be shutdown the instance on which this notebook is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd52a5",
   "metadata": {},
   "source": [
    "## 4. Conclusion <a id='Conclusion'></a>\n",
    "\n",
    "We have now seen how to use natural language queries to process CSV data using the LLMs hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/). Through this, you were able to learn how to configure model specific prompts, use prompt templates and observe the LLM behaviors for various prompts and call-to-action scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf266cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 5. Frequently Asked Questions (FAQs) <a id='FAQs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2c50f",
   "metadata": {},
   "source": [
    "**Q: What AWS services are used in this notebook?**\n",
    "\n",
    "Amazon Bedrock, AWS Identity and Access Management (IAM), Amazon CloudWatch, and Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook depending on what you use to run the notebook.\n",
    "\n",
    "**Q: Will Amazon Bedrock capture and store my data?**\n",
    "\n",
    "Amazon Bedrock doesn't use your prompts and continuations to train any AWS models or distribute them to third parties. Your training data isn't used to train the base Amazon Titan models or distributed to third parties. Other usage data, such as usage timestamps, logged account IDs, and other information logged by the service, is also not used to train the models.\n",
    "\n",
    "Amazon Bedrock uses the fine tuning data you provide only for fine tuning an Amazon Titan model. Amazon Bedrock doesn't use fine tuning data for any other purpose, such as training base foundation models.\n",
    "\n",
    "Each model provider has an escrow account that they upload their models to. The Amazon Bedrock inference account has permissions to call these models, but the escrow accounts themselves don't have outbound permissions to Amazon Bedrock accounts. Additionally, model providers don't have access to Amazon Bedrock logs or access to customer prompts and continuations.\n",
    "\n",
    "Amazon Bedrock doesn’t store or log your data in its service logs.\n",
    "\n",
    "**Q: What models are supported by Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n",
    "\n",
    "**Q: What is the difference between On-demand and Provisioned Throughput in Amazon Bedrock?**\n",
    "\n",
    "With the On-Demand mode, you only pay for what you use, with no time-based term commitments. For text generation models, you are charged for every input token processed and every output token generated. For embeddings models, you are charged for every input token processed. A token is comprised of a few characters and refers to the basic unit that a model learns to understand user input and prompt to generate results. For image generation models, you are charged for every image generated.\n",
    "\n",
    "With the Provisioned Throughput mode, you can purchase model units for a specific base or custom model. The Provisioned Throughput mode is primarily designed for large consistent inference workloads that need guaranteed throughput. Custom models can only be accessed using Provisioned Throughput. A model unit provides a certain throughput, which is measured by the maximum number of input or output tokens processed per minute. With this Provisioned Throughput pricing, charged by the hour, you have the flexibility to choose between 1-month or 6-month commitment terms.\n",
    "\n",
    "**Q: Where can I find customer references for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/testimonials/).\n",
    "\n",
    "**Q: Where can I find resources for prompt engineering?**\n",
    "\n",
    "[Prompt Engineering Guide](https://www.promptingguide.ai/).\n",
    "\n",
    "**Q: Is LangChain mandatory to use Amazon Bedrock?**\n",
    "\n",
    "No. You can interact with Amazon Bedrock using the [Bedrock API](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html) or language-specific [AWS SDKs](https://aws.amazon.com/developer/tools/). \n",
    "\n",
    "**Q: How do I get started with LangChain?**\n",
    "\n",
    "Go [here](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n",
    "**Q: Where can I find pricing information for the AWS services used in this notebook?**\n",
    "\n",
    "- Amazon Bedrock pricing - go [here](https://aws.amazon.com/bedrock/pricing/).\n",
    "- AWS Identity and Access Management (IAM) pricing - free.\n",
    "- Amazon CloudWatch pricing - go [here](https://aws.amazon.com/cloudwatch/pricing/).\n",
    "- Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook pricing - go [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc1023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
